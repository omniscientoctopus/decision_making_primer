{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by the 5-tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\gamma  \\rangle$ where,\n",
    "\n",
    "$\\mathcal{S}$ is the state space\n",
    "\n",
    "$\\mathcal{A}$ is the action space\n",
    "\n",
    "$\\mathcal{T}$ is the transition model\n",
    "\n",
    "$\\mathcal{R}$ is the reward model\n",
    "\n",
    "$\\gamma$ is the discount factor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state \t time \t damage state\n",
      "  1:  \t  1 \t     1\n",
      "  2:  \t  1 \t     2\n",
      "  3:  \t  1 \t     3\n",
      "  4:  \t  1 \t     4\n",
      "  5:  \t  1 \t     5\n",
      "  6:  \t  2 \t     1\n",
      "  7:  \t  2 \t     2\n",
      "  8:  \t  2 \t     3\n",
      "  9:  \t  2 \t     4\n",
      "  10:  \t  2 \t     5\n",
      "  11:  \t  3 \t     1\n",
      "  12:  \t  3 \t     2\n",
      "  13:  \t  3 \t     3\n",
      "  14:  \t  3 \t     4\n",
      "  15:  \t  3 \t     5\n"
     ]
    }
   ],
   "source": [
    "TIME_HORIZON = 3\n",
    "num_damage_states = 5\n",
    "\n",
    "num_states = TIME_HORIZON * num_damage_states\n",
    "\n",
    "state_space = {}\n",
    "key = 1\n",
    "\n",
    "print('state \\t time \\t damage state')\n",
    "for time in range(1, TIME_HORIZON+1):\n",
    "    for damage_state in range(1, num_damage_states+1):\n",
    "        state_space[key] = (time, damage_state)\n",
    "        \n",
    "        print(f'  {key}:  \\t  {time} \\t     {damage_state}')\n",
    "\n",
    "        key += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The action space has 3 actions and we carry out these actions at the beginning of the time-step\n",
    "\n",
    "0: Do nothing - component undergoes deterioration due to the environment\n",
    "\n",
    "1: Repair - component moves back by 1 damage state but remains failed if it has failed (+undergoes deterioration due to the environment)\n",
    "\n",
    "2: Replace - component is replaced (+undergoes deterioration due to the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NOTHING = 0\n",
    "MINOR_REPAIR = 1\n",
    "REPLACE = 2\n",
    "\n",
    "action_space = [DO_NOTHING, MINOR_REPAIR, REPLACE]\n",
    "\n",
    "num_actions = len(action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition model specifies the probability of transitioning from state ($s$) to state ($s'$) for a given action ($a$). \n",
    "\n",
    "For action $a =$ Do-Nothing, the transition model summarises the deterioration process.\n",
    "\n",
    "Mathematically, the probability of transitioning from state ($s$) to state ($s'$) for given action ($a$)\n",
    "is written as: $\\mathbb{P}(s' | s, a)$\n",
    "\n",
    "The transition model for discrete state spaces can be written as a matrix and the elements of the matrix are the probabilities of transitioning from state ($s$) to state ($s'$) for given action ($a$).\n",
    "\n",
    "More formally, $T^a_{i, j} = \\mathbb{P}(s_j | s_i, a)$\n",
    "\n",
    "Example: Consider the action $a=$ Do-Nothing. The probability of transitioning from damage state $s$ to\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 2: Can you explain why the matrix element (4,4) is 1 in for action 'minor-repair' in TRANSITION_MODEL[1]?** \n",
    "\n",
    "(Hint: Look at the definition of the action 'minor-repair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITION_MODEL = np.zeros((num_actions, num_damage_states, num_damage_states))\n",
    "\n",
    "# action[0] = do-nothing\n",
    "TRANSITION_MODEL[0] = np.array([[0.7, 0.3, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.6, 0.4, 0.0, 0.0],\n",
    "                                [0.0, 0.0, 0.5, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 0.0, 0.2, 0.8],\n",
    "                                [0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "# action[1] = minor-repair\n",
    "TRANSITION_MODEL[1] = np.array([[1, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0],\n",
    "                                [0, 1, 0, 0, 0],\n",
    "                                [0, 0, 1, 0, 0],\n",
    "                                [0, 0, 0, 0, 1]])\n",
    "\n",
    "# action[2] = replace\n",
    "TRANSITION_MODEL[2] = np.array([[1, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0],\n",
    "                                [1, 0, 0, 0, 0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPAIR_COST = -25\n",
    "REPLACE_COST = -50\n",
    "REWARDS = [0, REPAIR_COST, REPLACE_COST]\n",
    "\n",
    "PENALTY = -500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_model(current_state, action):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    current_state : tuple\n",
    "        (current_time, current_damage_state)\n",
    "\n",
    "    action : int\n",
    "        (1, 2, or 3)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output: list of tuples\n",
    "            Each tuple is of the form: (p(s'|s, a), s', r)\n",
    "            s' is the next state s\n",
    "            p(s'|s, a) is the probability of trasitioning to state s' given the current state s and the action a\n",
    "            r is the reward \n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    current_time, current_damage_state = current_state\n",
    "    next_time = current_time + 1\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    # action = 'do-nothing'\n",
    "    # damage state does not change\n",
    "\n",
    "    # action = 'minor-repair'\n",
    "    if action == 1:\n",
    "        # move back by one state\n",
    "        # but not lower than 1\n",
    "        # but no minor repair for failure\n",
    "        if current_damage_state != 5:\n",
    "            current_damage_state = max(1, current_damage_state-1)\n",
    "\n",
    "    # action = 'replace'\n",
    "    elif action == 2:\n",
    "        # replacing leads to initial undamaged state\n",
    "        current_damage_state = 1\n",
    "\n",
    "    for next_damage_state in range(1, num_damage_states+1):\n",
    "\n",
    "        next_state = (next_time, next_damage_state)\n",
    "        prob = TRANSITION_MODEL[0][current_damage_state-1, next_damage_state-1]\n",
    "        reward = REWARDS[action]\n",
    "\n",
    "        if next_damage_state == 5:\n",
    "            reward += PENALTY\n",
    "\n",
    "        output.append((prob, next_state, reward))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Below is an example of how the above function can be used.\n",
    "\n",
    "Assume that we are in the state 5 (time: 1, damage_state:5) and take action (2: replace).\n",
    "\n",
    "We know that, actions are taken at the beginning of the time step, i.e. the component is replaced \n",
    "and the new component undergoes deterioration at the end of the time step. \n",
    "\n",
    "Therefore, the next time step is 2 and the possible damage states are 0 or 1 with probability 0.7 and 0.3 respectively (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: (1, 5)\n",
      "Action: 2 \n",
      "\n",
      "Next state: (2, 1), probability: 0.7, reward: -50\n",
      "Next state: (2, 2), probability: 0.3, reward: -50\n",
      "Next state: (2, 3), probability: 0.0, reward: -50\n",
      "Next state: (2, 4), probability: 0.0, reward: -50\n",
      "Next state: (2, 5), probability: 0.0, reward: -550\n"
     ]
    }
   ],
   "source": [
    "current_state = state_space[5]\n",
    "# action = 0 # do nothing\n",
    "# action = 1 # minor-repair\n",
    "action = 2 # replace\n",
    "\n",
    "print(f\"Current state: {current_state}\")\n",
    "print(f\"Action: {action} \\n\")\n",
    "\n",
    "# returns a list p(s'|s,a), s' and r\n",
    "output = MDP_model(current_state, action)\n",
    "\n",
    "for prob, next_state, reward in output:\n",
    "    print(f\"Next state: {next_state}, probability: {prob}, reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (1, 1),\n",
       " 2: (1, 2),\n",
       " 3: (1, 3),\n",
       " 4: (1, 4),\n",
       " 5: (1, 5),\n",
       " 6: (2, 1),\n",
       " 7: (2, 2),\n",
       " 8: (2, 3),\n",
       " 9: (2, 4),\n",
       " 10: (2, 5),\n",
       " 11: (3, 1),\n",
       " 12: (3, 2),\n",
       " 13: (3, 3),\n",
       " 14: (3, 4),\n",
       " 15: (3, 5)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: (3, 5)\n",
      "Action: 2 \n",
      "\n",
      "Next state: (4, 1), probability: 0.7, reward: -50\n",
      "Next state: (4, 2), probability: 0.3, reward: -50\n",
      "Next state: (4, 3), probability: 0.0, reward: -50\n",
      "Next state: (4, 4), probability: 0.0, reward: -50\n",
      "Next state: (4, 5), probability: 0.0, reward: -550\n"
     ]
    }
   ],
   "source": [
    "current_state = state_space[15]\n",
    "# action = 0 # do nothing\n",
    "# action = 1 # minor-repair\n",
    "action = 2 # replace\n",
    "\n",
    "print(f\"Current state: {current_state}\")\n",
    "print(f\"Action: {action} \\n\")\n",
    "\n",
    "# returns a list p(s'|s,a), s' and r\n",
    "output = MDP_model(current_state, action)\n",
    "\n",
    "for prob, next_state, reward in output:\n",
    "    print(f\"Next state: {next_state}, probability: {prob}, reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df9c06b5e8521cf4ac10b32b5194d6e2193b759c220900fa5a35003364e8f855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
