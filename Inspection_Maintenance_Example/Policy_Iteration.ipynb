{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by the tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\gamma  \\rangle$ where,\n",
    "\n",
    "$\\mathcal{S}$ is the state space\n",
    "\n",
    "$\\mathcal{A}$ is the action space\n",
    "\n",
    "$\\mathcal{T}$ is the transition model\n",
    "\n",
    "$\\mathcal{R}$ is the reward model\n",
    "\n",
    "$\\gamma$ is the discount factor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (1, 1), 2: (1, 2), 3: (1, 3), 4: (1, 4), 5: (1, 5), 6: (2, 1), 7: (2, 2), 8: (2, 3), 9: (2, 4), 10: (2, 5), 11: (3, 1), 12: (3, 2), 13: (3, 3), 14: (3, 4), 15: (3, 5), 16: (4, 1), 17: (4, 2), 18: (4, 3), 19: (4, 4), 20: (4, 5), 21: (5, 1), 22: (5, 2), 23: (5, 3), 24: (5, 4), 25: (5, 5), 26: (6, 1), 27: (6, 2), 28: (6, 3), 29: (6, 4), 30: (6, 5), 31: (7, 1), 32: (7, 2), 33: (7, 3), 34: (7, 4), 35: (7, 5), 36: (8, 1), 37: (8, 2), 38: (8, 3), 39: (8, 4), 40: (8, 5), 41: (9, 1), 42: (9, 2), 43: (9, 3), 44: (9, 4), 45: (9, 5), 46: (10, 1), 47: (10, 2), 48: (10, 3), 49: (10, 4), 50: (10, 5)}\n"
     ]
    }
   ],
   "source": [
    "TIME_HORIZON = 10\n",
    "num_damage_states = 5\n",
    "\n",
    "num_states = TIME_HORIZON * num_damage_states\n",
    "\n",
    "state_space = {}\n",
    "key = 1\n",
    "\n",
    "for time in range(1, TIME_HORIZON+1):\n",
    "    for state in range(1, num_damage_states+1):\n",
    "        state_space[key] = (time, state)\n",
    "        key += 1\n",
    "\n",
    "print(state_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The action space has 3 actions and we carry out these actions at the beginning of the time-step\n",
    "\n",
    "0: Do nothing - component undergoes deterioration due to the environment\n",
    "\n",
    "1: Repair - component moves back by 1 damage state (+undergoes deterioration due to the environment)\n",
    "\n",
    "2: Replace - component is replaced (+undergoes deterioration due to the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NOTHING = 0\n",
    "MINOR_REPAIR = 1\n",
    "REPLACE = 2\n",
    "\n",
    "action_space = [DO_NOTHING, MINOR_REPAIR, REPLACE]\n",
    "\n",
    "num_actions = len(action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITION_MODEL = np.array([[0.7, 0.3, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.6, 0.4, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.5, 0.5, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.1, 0.9],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 1.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPAIR_COST = -25\n",
    "REPLACE_COST = -50\n",
    "\n",
    "REWARDS = [0, REPAIR_COST, REPLACE_COST]\n",
    "\n",
    "PENALTY = -500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_model(current_state, action):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    current_state: \n",
    "        _description_\n",
    "\n",
    "    action:\n",
    "        \n",
    "    \"\"\"    \n",
    "\n",
    "    current_time, current_damage_state = current_state\n",
    "    next_time = current_time + 1\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    # action = 'do-nothing'\n",
    "    # damage state does not change\n",
    "\n",
    "    # action = 'minor-repair'\n",
    "    if action == 1:\n",
    "        # move back by one state\n",
    "        # but not lower than 1\n",
    "        # but no minor repair for failure\n",
    "        if current_damage_state != 5:\n",
    "            current_damage_state = max(1, current_damage_state-1)\n",
    "\n",
    "    # action = 'replace'\n",
    "    elif action == 2:\n",
    "        # replacing leads to initial undamaged state\n",
    "        current_damage_state = 1\n",
    "\n",
    "    for next_damage_state in range(1, num_damage_states+1):\n",
    "\n",
    "        next_state = (next_time, next_damage_state)\n",
    "        prob = TRANSITION_MODEL[current_damage_state-1, next_damage_state-1]\n",
    "        reward = REWARDS[action]\n",
    "\n",
    "        if next_damage_state == 5:\n",
    "            reward += PENALTY\n",
    "\n",
    "        output.append((prob, next_state, reward))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7, (2, 1), -50),\n",
       " (0.3, (2, 2), -50),\n",
       " (0.0, (2, 3), -50),\n",
       " (0.0, (2, 4), -50),\n",
       " (0.0, (2, 5), -550)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDP_model(state_space[5], 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy):\n",
    "\n",
    "    # Initialise values of all states with 0\n",
    "    value_function_pi = np.zeros(num_states)\n",
    "\n",
    "    delta_threshold = 1e-5\n",
    "    delta = 10\n",
    "\n",
    "    while delta > delta_threshold:\n",
    "        delta = 0\n",
    "\n",
    "        # ignore terminal states, since value is 0\n",
    "        for idx_state in range(num_states-num_damage_states):\n",
    "\n",
    "            state = state_space[idx_state+1]\n",
    "            old_value = value_function_pi[idx_state]\n",
    "\n",
    "            # get action from policy \n",
    "            action = policy[idx_state]\n",
    "\n",
    "            new_value = 0\n",
    "            for tuple in MDP_model(state, action):\n",
    "                prob, next_state, reward = tuple\n",
    "                idx_next_state = (next_state[0]-1) * 5 + next_state[1]-1\n",
    "                new_value += prob * (reward + DISCOUNT_FACTOR * value_function_pi[idx_next_state])\n",
    "\n",
    "            value_function_pi[idx_state] = new_value\n",
    "\n",
    "            delta = max([delta, np.abs(old_value - new_value)])\n",
    "    \n",
    "    return value_function_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy improvement\n",
    "q_values = np.zeros((num_states, len(action_space)))\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "policy_stable = False\n",
    "\n",
    "while policy_stable == False:\n",
    "\n",
    "    value_function_pi = evaluate_policy(policy)\n",
    "    policy_stable = True\n",
    "\n",
    "    # ignore terminal states, since value is 0\n",
    "    for idx_state in range(num_states-num_damage_states):\n",
    "\n",
    "        # get old action from policy \n",
    "        old_action = policy[idx_state]\n",
    "        state = state_space[idx_state+1]\n",
    "\n",
    "        # store Q values of all actions\n",
    "        for idx_action, action in enumerate(action_space):\n",
    "            q_val = 0\n",
    "            for tuple in MDP_model(state, action):\n",
    "                prob, next_state, reward = tuple\n",
    "                idx_next_state = (next_state[0]-1) * 5 + next_state[1]-1\n",
    "                q_val += prob * (reward + DISCOUNT_FACTOR * value_function_pi[idx_next_state])\n",
    "            q_values[idx_state, idx_action] = q_val\n",
    "\n",
    "        # break ties evenly\n",
    "        best_actions = np.flatnonzero(q_values[idx_state, :] == max(q_values[idx_state, :]))\n",
    "        policy[idx_state] = np.random.choice(best_actions)\n",
    "\n",
    "        if old_action != policy[idx_state]:\n",
    "            policy_stable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.reshape(TIME_HORIZON, num_damage_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of states: [time_horizon, num_states] \n",
      " [[-16.602 -33.016 -49.573 -66.602 -66.602]\n",
      " [-13.485 -30.024 -46.677 -63.485 -63.485]\n",
      " [-10.054 -26.484 -43.672 -60.054 -60.054]\n",
      " [ -6.519 -22.024 -40.531 -56.519 -56.519]\n",
      " [ -3.357 -16.312 -36.711 -53.357 -53.357]\n",
      " [ -1.094  -9.882 -30.488 -51.094 -51.094]\n",
      " [  0.     -4.05  -21.375 -46.375 -50.   ]\n",
      " [  0.      0.    -11.25  -36.25  -50.   ]\n",
      " [  0.      0.      0.    -25.    -50.   ]\n",
      " [  0.      0.      0.      0.      0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of states: [time_horizon, num_states] \\n {np.around(value_function_pi.reshape(TIME_HORIZON, num_damage_states),3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df9c06b5e8521cf4ac10b32b5194d6e2193b759c220900fa5a35003364e8f855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
