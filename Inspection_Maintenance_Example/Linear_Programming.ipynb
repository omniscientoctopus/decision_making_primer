{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "An MDP is defined by the tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\gamma  \\rangle$ where,\n",
    "\n",
    "$\\mathcal{S}$ is the state space\n",
    "\n",
    "$\\mathcal{A}$ is the action space\n",
    "\n",
    "$\\mathcal{T}$ is the transition model\n",
    "\n",
    "$\\mathcal{R}$ is the reward model\n",
    "\n",
    "$\\gamma$ is the discount factor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (1, 1), 2: (1, 2), 3: (1, 3), 4: (1, 4), 5: (1, 5), 6: (2, 1), 7: (2, 2), 8: (2, 3), 9: (2, 4), 10: (2, 5), 11: (3, 1), 12: (3, 2), 13: (3, 3), 14: (3, 4), 15: (3, 5), 16: (4, 1), 17: (4, 2), 18: (4, 3), 19: (4, 4), 20: (4, 5), 21: (5, 1), 22: (5, 2), 23: (5, 3), 24: (5, 4), 25: (5, 5), 26: (6, 1), 27: (6, 2), 28: (6, 3), 29: (6, 4), 30: (6, 5), 31: (7, 1), 32: (7, 2), 33: (7, 3), 34: (7, 4), 35: (7, 5), 36: (8, 1), 37: (8, 2), 38: (8, 3), 39: (8, 4), 40: (8, 5), 41: (9, 1), 42: (9, 2), 43: (9, 3), 44: (9, 4), 45: (9, 5), 46: (10, 1), 47: (10, 2), 48: (10, 3), 49: (10, 4), 50: (10, 5)}\n"
     ]
    }
   ],
   "source": [
    "TIME_HORIZON = 10\n",
    "num_damage_states = 5\n",
    "\n",
    "num_states = TIME_HORIZON * num_damage_states\n",
    "\n",
    "state_space = {}\n",
    "key = 1\n",
    "\n",
    "for time in range(1, TIME_HORIZON+1):\n",
    "    for state in range(1, num_damage_states+1):\n",
    "        state_space[key] = (time, state)\n",
    "        key += 1\n",
    "\n",
    "print(state_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The action space has 3 actions and we carry out these actions at the beginning of the time-step\n",
    "\n",
    "0: Do nothing - component undergoes deterioration due to the environment\n",
    "\n",
    "1: Repair - component moves back by 1 damage state (+undergoes deterioration due to the environment)\n",
    "\n",
    "2: Replace - component is replaced (+undergoes deterioration due to the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NOTHING = 0\n",
    "MINOR_REPAIR = 1\n",
    "REPLACE = 2\n",
    "\n",
    "action_space = [DO_NOTHING, MINOR_REPAIR, REPLACE]\n",
    "\n",
    "num_actions = len(action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITION_MODEL = np.array([[0.7, 0.3, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.6, 0.4, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.5, 0.5, 0.0],\n",
    "                             [0.0, 0.0, 0.0, 0.2, 0.8],\n",
    "                             [0.0, 0.0, 0.0, 0.0, 1.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPAIR_COST = -25\n",
    "REPLACE_COST = -50\n",
    "\n",
    "REWARDS = [0, REPAIR_COST, REPLACE_COST]\n",
    "\n",
    "PENALTY = -500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_model(current_state, action):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    current_state: \n",
    "        _description_\n",
    "\n",
    "    action:\n",
    "        \n",
    "    \"\"\"    \n",
    "\n",
    "    current_time, current_damage_state = current_state\n",
    "    next_time = current_time + 1\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    # action = 'do-nothing'\n",
    "    # damage state does not change\n",
    "\n",
    "    # action = 'minor-repair'\n",
    "    if action == 1:\n",
    "        # move back by one state\n",
    "        # but not lower than 1\n",
    "        # but no minor repair for failure\n",
    "        if current_damage_state != 5:\n",
    "            current_damage_state = max(1, current_damage_state-1)\n",
    "\n",
    "    # action = 'replace'\n",
    "    elif action == 2:\n",
    "        # replacing leads to initial undamaged state\n",
    "        current_damage_state = 1\n",
    "\n",
    "    for next_damage_state in range(1, num_damage_states+1):\n",
    "\n",
    "        next_state = (next_time, next_damage_state)\n",
    "        prob = TRANSITION_MODEL[current_damage_state-1, next_damage_state-1]\n",
    "        reward = REWARDS[action]\n",
    "\n",
    "        if next_damage_state == 5:\n",
    "            reward += PENALTY\n",
    "\n",
    "        output.append((prob, next_state, reward))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7, (2, 1), -50),\n",
       " (0.3, (2, 2), -50),\n",
       " (0.0, (2, 3), -50),\n",
       " (0.0, (2, 4), -50),\n",
       " (0.0, (2, 5), -550)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDP_model(state_space[5], 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Programming (Primal formulation)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear programming: minimize a linear objective function subject to linear equality and inequality constraints.\n",
    "Linear programming solves problems of the following form:\n",
    "\n",
    "$\n",
    "\\begin{aligned} \n",
    "\\underset{v \\in \\mathbb{R}^{|\\mathcal{S}|}}{\\operatorname{minimize}} & \\quad \\sum_{ s} c(s) v(s) \\\\ \n",
    "\\text { subject to } & \\quad v(s) \\geq  \\sum_{s^{\\prime}} P\\left(s^{\\prime} \\mid s, a\\right) \\Big [ r(s, a, s') + \\gamma \\cdot  v\\left(s^{\\prime}\\right)\\Big ], \\quad \\forall a \\in \\mathcal{A}, s \\in \\mathcal{S} \\\\ & \\quad v(s) \\text { unconstrained }, \\forall s \\in \\mathcal{S}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "The optimization problem is an LP with $|\\mathcal{S}|$ variables and $|\\mathcal{S}| \\times|\\mathcal{A}|$ constraints. We rewrite the above LP in the standard formulation as:\n",
    "\n",
    "$\\begin{aligned} \\underset{v \\in \\mathbb{R}^{|\\mathcal{S}|}}{\\operatorname{minimize}} & \\quad \\sum_{s} c(s) v(s) \\\\ \n",
    "\\text { subject to } & \\quad - v(s) + \\gamma \\cdot \\sum_{s^{\\prime}}  P\\left(s^{\\prime} \\mid s, a\\right) \\cdot  v\\left(s^{\\prime}\\right)  \\leq - \\sum_{s^{\\prime}} P\\left(s^{\\prime} \\mid s, a\\right) \\cdot r(s, a, s'), \\quad \\forall a \\in \\mathcal{A}, s \\in \\mathcal{S} \\\\ & \\quad v(s) \\text { unconstrained }, \\forall s \\in \\mathcal{S}\\end{aligned}$\n",
    "\n",
    "([Adapted from \"Linear Programming in Reinforcement learning\"](https://escholarship.mcgill.ca/downloads/xs55mh725))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [scipy.optimize.linprog](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html) module to solve the problem and use the same variable notation for convinience.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\underset{v}{\\operatorname{minimize}} \\quad & c^T v \\\\\n",
    "\\text { such that } & A_{u b} v \\leq b_{u b}, \\\\\n",
    "& A_{e q} v=b_{e q}, \\\\\n",
    "& l \\leq v \\leq u,\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "where $v$ is a vector of value of states; $c, b_{u b}, b_{e q}, l$, and $u$ are vectors; and $A_{u b}$ and $A_{e q}$ are matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inequality constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we exclude the terminal states here and include them in equality constraints next\n",
    "num_constraints = (num_states-num_damage_states) * num_actions\n",
    "\n",
    "# mu(s) in the above formulation\n",
    "c = np.ones(num_states)\n",
    "\n",
    "# Matrix corresponding to the inequality constraints\n",
    "A_ub = np.zeros((num_constraints, num_states))\n",
    "\n",
    "# Vector corresponding to the inequality constraints\n",
    "b_ub = np.zeros(num_constraints)\n",
    "\n",
    "k = 0 # counter for counting constraint\n",
    "\n",
    "# loop over all states except terminal states\n",
    "for idx_state in range(num_states-num_damage_states):\n",
    "    # loop over all actions in action space\n",
    "    for action in action_space:\n",
    "        \n",
    "        A_ub[k, idx_state] = -1\n",
    "\n",
    "        state = state_space[idx_state+1]\n",
    "\n",
    "        # enumerate all (one-step) future states given a (state, action) pair\n",
    "        for prob, next_state, reward in MDP_model(state, action):\n",
    "            \n",
    "            # compute global index of state \n",
    "            idx_next_state = (next_state[0]-1) * num_damage_states + next_state[1]-1\n",
    "\n",
    "            # add entries in corresponding next states\n",
    "            A_ub[k, idx_next_state] = DISCOUNT_FACTOR * prob \n",
    "            b_ub[k] += - prob * reward\n",
    "\n",
    "        k += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equality constraints for terminal states\n",
    "A_eq = np.zeros((num_damage_states, num_states))\n",
    "\n",
    "# set coefficient of terminal states to 1\n",
    "for idx_state in range(num_damage_states):\n",
    "    A_eq[idx_state, -1 - idx_state] = 1\n",
    "\n",
    "# value of terminal states is 0\n",
    "b_eq = np.zeros(num_damage_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Linear Program converged?: True\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we must specify bounds for the unconstrained LP since default is (0, None) (non-negative)\n",
    "result = linprog(c, \n",
    "                 A_ub=A_ub, \n",
    "                 b_ub=b_ub, \n",
    "                 A_eq=A_eq, \n",
    "                 b_eq=b_eq, \n",
    "                 bounds=(-math.inf, math.inf))\n",
    "\n",
    "print(f\"Has Linear Program converged?: {result.success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of states: [time_horizon, num_states] \n",
      " [[-16.602 -33.016 -49.573 -66.602 -66.602]\n",
      " [-13.485 -30.024 -46.677 -63.485 -63.485]\n",
      " [-10.054 -26.484 -43.672 -60.054 -60.054]\n",
      " [ -6.519 -22.024 -40.531 -56.519 -56.519]\n",
      " [ -3.357 -16.312 -36.711 -53.357 -53.357]\n",
      " [ -1.094  -9.882 -30.488 -51.094 -51.094]\n",
      " [ -0.     -4.05  -21.375 -46.375 -50.   ]\n",
      " [ -0.     -0.    -11.25  -36.25  -50.   ]\n",
      " [ -0.     -0.     -0.    -25.    -50.   ]\n",
      " [ -0.     -0.     -0.     -0.     -0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of states: [time_horizon, num_states] \\n {np.around(result.x.reshape(TIME_HORIZON, num_damage_states),3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal policy\n",
    "\n",
    "The constraint corresponding to the best action will be \"tight\" or alternatively, will have \"no slack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get slack of the inequality constraints\n",
    "optimal_policy = result.slack.reshape((num_states-num_damage_states),  num_actions)\n",
    "\n",
    "# arguments of actions having no (minimum) slack\n",
    "optimal_policy = np.argmin(optimal_policy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 2, 2],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 1, 2],\n",
       "       [0, 0, 0, 1, 2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal policy (excluding terminal states)\n",
    "optimal_policy.reshape(TIME_HORIZON-1, num_damage_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df9c06b5e8521cf4ac10b32b5194d6e2193b759c220900fa5a35003364e8f855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
