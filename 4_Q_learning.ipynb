{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solving MDPs using Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Given an MDP $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\gamma \\rangle$, we would like to find an optimal policy $\\pi^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the dynamic programming approaches like Value Iteration and Policy Iteration, we were able to *plan* our actions when we know the probabilities over the next states, given by $\\mathcal{T}$ and rewards, given by $\\mathcal{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> \n",
    "<img src=\"images/backup_diagram.png\"  width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Practical challenge: $\\mathcal{T}, \\mathcal{R}$ not explicitly available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"> \n",
    "<img src=\"images/RL_crow.gif\"  width=\"40%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Real environments can be very complex and you might not have an accurate model to describe it.\n",
    "\n",
    "For example, training robots, develop self-driving cars and many more!\n",
    "\n",
    "However, you can interact with the environment to learn how to act optimally in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"> \n",
    "<img src=\"images/ml_paradigms.png\"  width=\"60%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- 3 Pillars in ML\n",
    "- RL: Decision-making under uncertainty -> MDP -> T and R not known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reinforcement Learning (RL)  focuses on *learning* the optimal actions by interacting with the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- Reinforcement Learning (RL)  focuses on *learning* the optimal actions by interacting with the environment. \n",
    "\n",
    "<div align=\"center\"> \n",
    "<img src=\"images/RL_illustration_0.png\"  width=\"50%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does it work?\n",
    "\n",
    "- Reinforcement Learning (RL)  focuses on *learning* the optimal actions by interacting with the environment. \n",
    "\n",
    "<div align=\"center\"> \n",
    "<img src=\"images/RL_illustration.png\"  width=\"50%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent is in state $s \\in \\mathcal{S}$ and choses to take action $a \\in \\mathcal{A}$ in the environment. This action results in reward $r \\in \\mathcal{R}$ and sends the agent into a new state $s'\\in \\mathcal{S}$ and the cycle continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"> \n",
    "<img src=\"images/rl_algorithms.png\"  width=\"70%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Learn $Q^*(s,a)$. But why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Act greedily wrt $Q^*(s,a)$,\n",
    "\n",
    "$$\\pi^*(s) = \\textrm{argmax}_{a} Q^*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall once again, the Bellman optimality equations for optimal $Q$ values,\n",
    "\n",
    "$$Q_{*}(s,a) = \\sum_{s'} \\mathcal{T}(s' | s, a) \\big [\\mathcal{R}(s,a,s') + \\gamma \\cdot \\max_{a'} Q_{*}(s',a') \\big ],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "If you look closely, we are seeking an estimate of the $Q$ value for a state-action pair which is of the form,\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{x} \\mathbb{P}(x) \\cdot x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Give yourself a cookie if you recognised this equation from the previous tutorials on sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quick Detour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the simplest Monte Carlo approach, we can estimate the mean as,\n",
    "\n",
    "$$\\mathbb{E}[X] \\approx \\mu_N(X) = \\frac{1}{N} \\sum_{i=1}^{N} x_i, \\, x_i \\sim X.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, say we are tracking the price $X$ of Apple's stock everyday, and in the first 5 days, we receive the following 5 values,\n",
    "\n",
    "$$152, 155, 157, 160, 158.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "What is the average value of the stock over the $5$ days?\n",
    "\n",
    "$$\\mu_5 = \\frac{152 + 155 + 157 + 160 + 158}{5}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the next trading day, we get a new update say the price is $159$. Simplest way to update our estimate is,\n",
    "\n",
    "$$\\mu_6 = \\frac{152 + 155 + 157 + 160 + 158 + 159}{6}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More clever way to do this,\n",
    "\n",
    "$$\\mu_6 = \\frac{(152 + 155 + 157 + 160 + 158) + 159}{6} = \\frac{\\mu_5 \\cdot 5 + 159}{6}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, \n",
    "\n",
    "$$\\mu_n =  \\frac{\\mu_{n-1} \\cdot (n-1) + x_n}{n},$$\n",
    "\n",
    "which is called a *cumulative average*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We rewrite this once again,\n",
    "\n",
    "$$\\mu_n =  \\Big ( 1 - \\frac{1}{n} \\Big ) \\mu_{n-1} + \\Big ( \\frac{1}{n} \\Big ) x_n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "General idea,\n",
    "\n",
    "$$\\mu_n = (1 - \\alpha) \\cdot \\text{old estimate} + \\alpha \\cdot \\text{new estimate},$$\n",
    "\n",
    "where, $\\alpha = \\frac{1}{n}$ for cumulative averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively,\n",
    "\n",
    "$$\\mu_n = \\text{old estimate} + \\alpha \\cdot (\\text{new estimate} - \\text{old estimate}),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back to Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we take actions in world, we collect experiences in the following form, \n",
    "\n",
    "$$(s, a, r, s').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then use these samples to improve our estimate of $Q$ values in the following way,\n",
    "\n",
    "$$Q_{k+1}(s, a) = (1 - \\alpha) \\cdot \\text{old estimate} + \\alpha \\cdot \\text{new estimate}$$\n",
    "\n",
    "where $\\alpha \\in (0, 1)$ is aptly called *learning rate* because it governs the rate at which we incorporate new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $\\alpha = 1 \\implies$ neglect old information\n",
    "\n",
    "- If $\\alpha = 0 \\implies$ neglect new information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We build the new estimate called *Temporal Difference Target* (TD target) using the Bellman equation,\n",
    "\n",
    "$$\\text{TD target} = r + \\gamma \\cdot V_{*}(s') $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But we don't have $V_{*}(s')$, we estimate the future value of $s'$ using our current estimate $Q_k$ (*bootstrapping*), like we did in DP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our TD target is therefore,\n",
    "\n",
    "$$\\text{TD target} = r + \\gamma \\cdot \\max_{a'} Q_k(s', a').$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $Q$ value update therefore has the following form,\n",
    "\n",
    "$$Q_{k+1}(s, a) = (1 - \\alpha) \\cdot \\text{old estimate} + \\alpha \\cdot \\text{TD target}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We now simply combine the previous equations together,\n",
    "\n",
    "$$Q_{k+1}(s, a) = (1 - \\alpha) \\cdot Q_k(s, a) + \\alpha \\cdot \\big [  r + \\gamma \\cdot \\max_{a'} Q_k(s', a') \\big ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$Q_{k+1}(s, a) = Q_k(s, a) - \\alpha \\cdot Q_k(s, a) + \\alpha \\cdot \\big [  r + \\gamma \\cdot \\max_{a'} Q_k(s', a') \\big ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$Q_{k+1}(s, a) = Q_k(s, a) + \\alpha \\cdot \\big [  r + \\gamma \\cdot \\max_{a'} Q_k(s, a') -  Q_k(s, a) \\big ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, we are nudging our current estimate in the direction of the difference between the current estimate and TD target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration v/s Exploitation Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optimal policy: $\\pi^*(s) = \\textrm{argmax}_{a} Q^*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, greedy policy: $\\pi_k(s) = \\textrm{argmax}_{a} Q_k(s,a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "rise": {
      "enable_chalkboard": true
     },
     "slide_type": "fragment"
    }
   },
   "source": [
    "Iteration 0:\n",
    "\n",
    "<div align=\"center\"> \n",
    "<img src=\"images/exploration.png\"  width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Iteration 1:\n",
    "\n",
    "<div align=\"center\"> \n",
    "<img src=\"images/exploration_1.png\"  width=\"30%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we are learning from experiences, we need to occasionally visit new states by deviating from the greedy policy by taking random actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Somewhat intuitively, you can think of this as follows,\n",
    "\n",
    "Consider the choice between watching a new TV show v/s rewatching your favourite TV show. You know that you will definitely derive pleasure from watching your favourite TV show. However, the new show could potentially be better than your current favourite but trying it out also carries the risk of being disappointed. At the same time, you will never find a better one if you do not explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploration Strategy: 𝜖-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The simplest policy is called the $\\epsilon$-greedy policy, where you take random actions with probability $\\epsilon$.\n",
    "\n",
    "Mathematically, the $\\epsilon$-greedy policy is given as follows,\n",
    "\n",
    "$$\n",
    "\\pi_{\\epsilon}(s) = \n",
    "\\begin{cases}\n",
    "  \\mathrm{argmax}_{a} Q_k(s, a), & \\mathbb{P} = 1 - \\epsilon, \\\\\n",
    "  a \\in \\mathcal{A}, & \\mathbb{P} = \\epsilon.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstration\n",
    "\n",
    "demonstration of various stages of Q learning, i.e. episode 0, episode 10, episode 50, episode 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Initialise Q values for all states-action pairs\n",
    "Choose a learning rate in (0, 1] # hyperparameter\n",
    "Choose exploration probability # hyperparameter\n",
    "Choose number of episodes # hyperparameter\n",
    "\n",
    "Loop for each episode:\n",
    "\n",
    "    Get start state s0\n",
    "\n",
    "    Loop until terminal state:\n",
    "\n",
    "        # choose an action\n",
    "        Choose actions according to the ε-greedy policy\n",
    "\n",
    "        # step in the environment\n",
    "        Take action (a) in the environment, observe reward (r) and move to next state (s')\n",
    "\n",
    "        # Q-learning using (s,a,r,s')\n",
    "        - get current value Q(s,a)\n",
    "        - estimate future value: V*(s')\n",
    "        - compute TD target: r + γ * V*(s')\n",
    "        - Apply learning update to Q(s,a)\n",
    "\n",
    "        Update current state to (s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from frozen_lake import FrozenLake\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_values, format_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = FrozenLake(prob_slip=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "# randomly initialise Q values for all states \n",
    "Q_values = np.random.rand(num_states, num_actions)\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.6\n",
    "NUM_EPISODES = 200\n",
    "\n",
    "store_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We train the agent for several episodes\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # ε-greedy policy to select actions\n",
    "        # select random action with probability ε\n",
    "        # |---------------ε----|\n",
    "        # 0                    1\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "        # select greedy action\n",
    "        # simply act greedily wrt Q values\n",
    "        else:\n",
    "            # get Q values of all actions in current state\n",
    "            q_values = Q_values[state, :]\n",
    "            # if more than one action provides maximum, \n",
    "            # we break ties randomly\n",
    "            optimal_actions = np.flatnonzero(q_values == max(q_values))\n",
    "            action = np.random.choice(optimal_actions)\n",
    "\n",
    "        # take a step in the environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        ## Q-Learning ##\n",
    "  \n",
    "        # get current Q values\n",
    "        old_q = Q_values[state, action]\n",
    "\n",
    "        # compute value of next_state\n",
    "        # V(s') = max_a' Q(s', a')\n",
    "        future_value = max(Q_values[next_state]) * int(1 - done)\n",
    "        \n",
    "        # compute TD target\n",
    "        # TD_target = r + γ max_a' Q(s', a')\n",
    "        TD_target = reward + DISCOUNT_FACTOR * future_value\n",
    "\n",
    "        # update Q table\n",
    "        # Q(s,a) = Q(s, a) + α (r + γ max_a' Q(s', a') - Q(s,a))\n",
    "        Q_values[state, action] = Q_values[state, action] + learning_rate * (TD_target - old_q)\n",
    "\n",
    "        # update the state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAGVCAYAAAAyrrwGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7uUlEQVR4nO3bd3wUdf7H8dcmm55NDy0kobeETgAFaaIUaaKCIHrc2bt4J96dd2cX9Wf3zgMs593ZKCIqKl2l95pKTYWEJJuebNru74/gxoVE8RwgwPv5eOSP/c5nyneG4T3znRmTw+FwICIiYiC3870BIiJy8VG4iIiI4RQuIiJiOIWLiIgYTuEiIiKGU7iIiIjhFC4iImI4hYuIiBhO4SIiIoYzn2nhqqH3ns3tkDNQXON+vjfhklekY3DeBZprz/cmXPKu2/jGz9bozkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDGc+3xtwplpPGkKbG0fiGRJA6eEskl9fSHFyWoO1LUcPJPZPN7u01VZWs/bqh5y/280cS4sRffFuFoy9ppbilHQOvfMlxUmpzpq2M0YRdlkslg6tsVfX8N24R85G1y4I7SZfQafpI/AOCaDoUBZ7Xl1MQVJ6g7XRY/vT77EZLm21ldUsHfF752+vYAux90ygef8uePj7kLfnMHtfXUxpZq6zxs3TTI/7rqX1yD64e5jJ2ZbE7pcWUVlQcnY62cR1vX4QsTeNwCfUQsHBY2x+eQl5iQ0fAwBPf2/63n0N0cN64BXgS2m2la2vLiVzUxIAvW8bRe/bR7vMU5iaw5Kpz7u0hcdG0/fuawiPicJhd2A9kMWKB+dRW1ltfCebOJ0HZ+6CCJfmw/vQ+d7JJL3yCUWJqUTdMJw+L93HxhlPUl1Y2uA81aUVbLr5qfoGh8NlennmCZJfX0jFsTzcvDyJ/mGZ05+guqhumSYPMznf7aIo4Sitxl521vrX1LW+sjc97r+W3f+3AGtiGh2nDGXwK/ewctozVP7E/l8x7Zn6Btfdz2XP34a9ppbNj75NdbmNjlOHM/j1e1l103PU2qoA6PnAZFpc1o2tf3mP6jIbvR6+noHP3cr3d792lnradLUd2Yv+D05i0wuLyE1II+bGoYx6/U4+nTIHW8Hpx8DN7M6oN+/GVlDK2j+9T3luIf4tQqgqrXCpKzh8nOX3/dP5215rd5keHhvNqNfvZN+/17DlpSXYa2sJ7RiBw+5adynQefDLXBDDYtFTriRz2SaOfbOFsrRskl7+hFpbFRE/9R++w0GVtbj+75SUz169A+vOFCqO51OWepyUfyzBw98HS/sIZ82Rf31F+qJvKTly7Gx17YLQcepwUr/cRNrXWylJzWbX/y2ktrKK6HEDG53H4XBQaS2p//vR/vePDCc0ti27X1pIQXI6pekn2P3SQty9PIi8qi8AZj9v2owbyL43l5K76yCFKRnsfPZDwnq0IySmzdnucpMTO20YKZ9v5uCybRQezWHj84uosVXRafyABus7jh+AV4Avqx95lxP7jlJ6vIDs3YexHnT9t2yvtVNhLXH+VRaVuUwfMGsSiQvXs+8/ayg8mk1xei5H1+zBXl171vraVOk8+GWa/J2LyeyOpVMkRz9cUd/ocGDdmUxgTLtG53P38WLwgqcxuZkoPpDBobe/oCz1eKPraD1+ENUl5ZQczjS6Cxc0k9mdoM6RpPx3VX2jw8GJHSmExrZtdD6zjxejP30Ck8lE4YFM4ud9ScnRbADcPOr+2dmralyWaa+qIbRHO1K/3Exw50jcPMyc2JHiLClJP0FZtpWQ2DZYE1IN7WdT5mZ2J7RLa/b+e3V9o8PBse0HCe8e3eA8UUNiOLE/lctnX0/UkFhsBaUcXrGL/f9dg8Nef/kcEBnGjcueoLaqhhP7U9nx1jLKcgoB8A72p1lsGw4v38U1bz9AQOswClNz2DX3a3L2Hj2bXW5ydB78ck3+zsUz0B83s/tpdx5VBSV4hQQ0OE95Rg6JL37AnsfmEf/M+5jcTMT94/d4hQe51IVdFsvwb17hylWvEXXDCHb94U2qT7lyu9R5BfnhZnbHZnXd/zZrCd4hlgbnKUk7wc45H7H5j2+z/an/YjKZGD53Fj4n939JWg5l2VZi7xyPh8UHk9mdTjeNxLd5MD6hdcfUOzSA2qoaqk8Zxqm0luDdyHG/WP1wDCpOOQYV1hJ8G9kXllahtBnRE5ObiZWz5rPnvZXE3jSMnr+92lmTm5DG+qc+ZsVD89j0wiL8W4Vwzbz7Mft61S0jIhSA3reP4sDnW1jx4DzyU7IY/fd7CIgMO0u9bZp0HvxyTT5c/hdFCUc5vmIbpYcyKdh7iL1/mU91YSmtxw92qbPuPsCW2+aw/d6Xyd+WSI8nbsUjyP88bfXFw5qQSvry7RQdzCJvzyE2//kdKgtLaTvpcgActXa2/Pld/KPCmbD8BSateYnwPh3J3pzgclUt/zuTmwlbQSkb5ywkPzmTo6v3sPdfq+gy+XJnTebmZFLX7qXg0HGytqawatZ8PC0+tL2yV90yTCYAUj7bxMFl27AeyGLba0spSjtBx0aG46TepX4eNPlhsaqiUuw1tXgGu14deAZbqLQWn9EyHLV2Sg5l4Ns63KXdbquiIiuXiqxcihJTGfTh40RcczmpH640bPsvdJWFZdhrak+7OvMOsZx2FdcYR62dwgOZ+EfU7//ClAzWzHwRs583bh5mqgpLGT7/YQqSMwCw5Rfj7mnGw9/H5arNK8SC7QyP+8Xih2Pgc8ox8AmxUN7IvijPK8ZRY3f5T6ooNQffsADczO7Ya05/ZlJVaqMoPdd5V1KeV7fswqM5LnWFqTn4Nw/+VX260Og8+OWa/J2Lo6aWkgMZhPTtXN9oMhHSpzNFCUfObCFuJvzbtqIyv+in60wm5zio1HHU1FKYkkF4v071jSYT4X07kx9/huPubiYC27fCln/6yVBTZqOqsBT/1uEEd4ni2Ib9ABSkZGCvrnFZr39UM/xahGCNT/01Xbrg2GtqyU/OpFWc6zFoFdeR3P0Nv45/Yt9RLK3D4OTdB0BAVDPKc4saDBYAs48nARGhVJwMldLjVspOFBIY3cylLjAqnNJs66/s1YVF58Evd0H8T5q2cA0xf7qF4uR0ipNTibp+BO4+Xhz7ZgsAMX++hcrcQg69/QUA7X4zhsKEo1Rk5WL296XNtJF4twgha9kmANy8PWl382hyN+6jMr8Yj0A/Iq8dildYEDnf7Xau17tZMOYAP3yaB2Nyd8O/Q2sAKrJyqa2oPMd74fw5uOBb+j02g4LkDAoS0+gwZRhmb0/SvtoKQL+/zKAir4iEuV8C0OW3o7EmpFKWmYuHvw+dpl+Jb4tgjn652bnMiOG9qCwspSKngIB2rej50GSOrd/HiW3JQN3JlrpsCz3uv5bq4vK6VzBnXU/+/qNN+iHm2RL/8Xdc8bfp5CVlkJtY9yqy2duTA8vqjsGQx6dTllvEzre+AiD50010veEKBj58LYkL1xMQFU7PmSNJXLDOucy4ByaQsT6B0mwrvmGB9L59NHa7gyMrdzlr9n/4LX1uH4314DHyD2TR8Zo4AqObsfZP75/T/jcFOg9+mQsiXHK+3YVnkIX2vxuHV4iFkkNZ7HrkH86H/N7NguFHt/9mf1+6PXITXiEWqksqKD6QzvZ7X6Ysre4tDex2fKOa02PU7XgG+lFdXEZRcjo7HnjF5Y2y9r8bR6sx9a8ZXvbunwDY8eBrFOw5eA563jRkrtmNV5A/3W4bW/fx2MFMNvz+n87XKn2bB+P40XdEnhYf+jx6I94hAVSXlFOQksG3d75GSWq2s8Y7NIAe91+Ld4iFivxi0pdvI+lfK1zWu/eNJfSwOxj47O9w8zCTsy2Z3S8tPDedbmKOrt6Dd5A/fe4YjU9oANYDWax8aB42a933FX7Ng12GwMpOFLLigbkMmDWJSR8+QnluEQmfrGP/f9c4a/yaBTLs6ZvxCvTDVlhKzt4jLLv1NWyF9S+1JH6yDrOnB/0fmohXgC/Wg8dY8cBcSrLyz13nmwidB7+MyeFwnNGTo1VD7z3b2yI/o7jG/XxvwiWvSMfgvAs0X3rf2DQ1121842drmvwzFxERufAoXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnPlMC4tr3M/mdsgZGD477HxvwiXvvacKz/cmXPIm3Ol9vjdBzoDuXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcObzvQFnqt3kK+g0fQTeIQEUHcpiz6uLKUhKb7A2emx/+j02w6WttrKapSN+7/ztFWwh9p4JNO/fBQ9/H/L2HGbvq4spzcx11rh5mulx37W0HtkHdw8zOduS2P3SIioLSs5OJ5uwxZtS+PD7JKwlFXRoGczDE/sRExXWaP2afWnMX7GP7IJSWodZuHdMby7vGuGc7nA4eHvlPr7YdoiSimp6tAln9rVxRIYHOGuKyit5ZekONiRl4mYyMax7FLMm9MXXy+Os9rWp6nXDIOJuGY5fqIXcg8dY8+JnZCc0fA5MnXcPkf06nNZ+ZEMiSx58BzezG4PvHkvbwV0JigihstRG2tYDrHvzK8ryip31zbpEMOT+cbSIicJRa+fA2n1898rnVFdUnbV+NmWf7jrCR1sPYS2rpEOzAGaN7EG3VsGN1q9NzuLt9clkF5XTOtiPu4fFcHn75s7pg174vMH57hnWjZsGdATgun+uJLu4wmX6XUO7cvPATgb06OwxORwOx5kUfjrogbO9LY1qfWVv+v3lZnb/3wKsiWl0nDKUiOG9WTntGSoLS0+rjx7bn54PXseKac/UNzpwCYVh82Zhr6ll/5tLqS630XHqcJoP7Mqqm56j1lZ34vT+wxRaXNaNHc9+SHWZjV4PX4/D7uD7u187211u0PDZjf9nfjat3pPKUws2M3tyf2KiwliwPpm1+9P55JHxhPh7n1a/LzWXe+au4q7RvRjcNYIVe1L54LtE3n9wDO1bBAHw328T+M+3Cfx16mW0CvFn/op9HM4u5KPfj8PLwx2AWe+uJb/YxqPX9aem1s4zC7fQNTKEp6YPPpfdd/HeU4XnZb2dr+rFmKems/q5RRyPT6fP9CF0HtmT9yY/T3nB6eeAd4Avbif3I4BPoC+/+fgPrHhmIQlfbsfT35sJL/yG/Uu3cOLAMbwtvox4ZBImNzc+uPlVAPzCApi5cDYpq3az86N1ePl5M/z3kyjLK+aLR/99zvp+qpn3np9r4tVJWTzz1S4eubouUBbuOMK3ycf4+PYrCfbzOq1+f6aVez/awJ1DuzKofQtWJmby4daD/GvmMNqdvIjKL7W5zLPlSA5zvtnDgjtHEhHkB9SFy7ge0UzoGe2s8/U04+N5/u4Nwn734s/WXBDDYh2nDif1y02kfb2VktRsdv3fQmorq4geN7DReRwOB5XWkvq/HwWLf2Q4obFt2f3SQgqS0ylNP8Hulxbi7uVB5FV9ATD7edNm3ED2vbmU3F0HKUzJYOezHxLWox0hMW3OdpeblI/XJzNhQAfGxbWnbfNAZk/uj5eHO8u2H26wfuGGZAZ0asmMYd1o0zyQO0f1pHNEMIs3pgB1x2bBhmRmXhnLkJhIOrQM5m9TLyOvuJx1CRkApOYUsSXlOH+6fgAxUWH0bNuMhyf1Y/XeNHKLys9Z35uKfjOGsv+zLcR/uZ38ozmsem4x1bZqYif2b7DeVlxOeX6J8y96QGeqbdUcWLUXgKpSG4vvnUfKqr0UpOVyPD6NNS8soUW3SCwnLwDaX9ENe00tq59fQkFaLtmJGayas5hOI3sS1Pr8XOicTwu2H2J8z2iu6RFN27AAHhnVs+482J/WYP3CnYcZ0K4ZNw3oSJswC3cM6Uqn5kEs3nXUWRPq7+3yt/5QNn2iw5zB8gNfT7NL3fkMljPV5MPFZHYnqHMkJ7an1Dc6HJzYkUJobNtG5zP7eDH60ycYs+RJLnv+dixtWzinuXnUHRh7VY3LMu1VNYT2aAdAcOdI3DzMnNhRv96S9BOUZVsJiW1jTOcuANU1taRkWYnr8KP952YirmML4tPyGpwnPj2PuI4tXdoGdGpFfHpd/TFrKfklNuI61i/T38eTbpFhzmXuT8/D4uNJ18hQZ01chxa4mUwkZOQb1r8LgZvZneZdWpO27UB9o8NB+rYDtOre5oyW0X3SAJJX7qba1vhwlqe/Nw67ncqSuiEYd08ztdU18KPBjRpbNQARvRs/9y5G1bV2UrKLiIsOd7a5mUz0axNOfFZBg/MkZBXQ70f1AAPaNiMhy9pgvbXMxqbDOYzrEX3atA+2HmTM618z81/f8eHWg9TY7b+iN+dGkw8XryA/3Mzu2Kyuzzls1hK8QywNzlOSdoKdcz5i8x/fZvtT/8VkMjF87ix8woNOTs+hLNtK7J3j8bD4YDK70+mmkfg2D8YntO521Ts0gNqqGqpLXcc6K60leIcEnLrKi1ZhWSW1dgchFtfhrxB/b/JLKhqcJ7/EdtpwWV29zTm9rs3HtcZSv8z8korThhrM7m4E+HhibWS9Fyufk+dAWb7rOVCWX4JfWMPnwI+1iIkivENL9i/d2miNu6eZIQ+MI2nFbqrKKgFI334Qv7AA4m4ejpvZHS+LD0PuvwaoGzK7lBSWV1LrcBByyr/JEF8vrGW2BufJL7OdXu/nRf7J/Xuqb+Iz8PU0M7ST64XZDX3b8eSEfrw5bRATe0Xz380HeevbxF/Rm3Oj6d9b/Q+sCalYE1Kdv/P3H+Hqjx6j7aTLSXz7axy1drb8+V36/mkaE5a/gL2mlhM7DpC9OQEwnbftFjkbuk8cQO7BY40+/HczuzH++VswmUysnrPY2Z5/JIdvHv+Y4bMmcMV9Y7HbHez+ZH3dA3/7GT2qlV9g2b50ru7WGi+zu0v7jf3rX8zo0CwQD3c3Xlyxl7uGdsXzlNqmpMmHS2VhGfaa2tPuUrxDLKfdzTTGUWun8EAm/hH1t6iFKRmsmfkiZj9v3DzMVBWWMnz+wxQk14352/KLcfc04+Hv43L34hViwWYtPm0dF6sgPy/c3UxYS1yvzqylNkItPg3OE2rxxlraUL23c3pdWwVhAfXLsJbY6HTyzZtQiw8Fp1zh1dTaKa6oIqSR9V6sKk6eA36hrueAX6iFsryfPgc8vD3pMqoXG+cub3B6XbD8hoCWISy86y3nXcsPkpfvInn5LnxD/OveEHNA35uGUph1aQ1NBvl64W4yYT1l/1jLKwnxO/2lFoBQP+/T68sqCW3g4f+ejHzSraU8NbHfz25Lt5bB1NodHC8qJzr05+9cz5cmPyzmqKmlMCWD8H4/eu3OZCK8b2fy4482PuOPuZkIbN8KW/7poVBTZqOqsBT/1uEEd4ni2Ib9ABSkZGCvrnFZr39UM/xahGCNT/01XbqgeJjd6RwRwo5D2c42u93BjkPZxEY3/FA3NirMpR5g28HjxJ58dblViD+hFm92HMxxTi+zVZOYkedcZveoMEoqqkjOrP9PbOfhHOwOBzE/eg5zKbDX1JKTnElUXMf6RpOJqLiOHNuf+pPzdrqqJ+4eZhK/3nnatB+CJTgyjEV3/xPbT7woUW4tpbqiis5X96K2qpq0LSmN1l6MPNzd6NwikB1p9Z8q2B0OdqbmEhvR8KvIMRHB7PxRPcD21BPERIScVrtsXxqdWwTSsVngz27LwRNFuJlo8A21pqTJ37kAHFzwLf0em0FBcgYFiWl0mDIMs7cnaV/VjSH3+8sMKvKKSJj7JQBdfjsaa0IqZZm5ePj70Gn6lfi2CObol5udy4wY3ovKwlIqcgoIaNeKng9N5tj6fZzYlgzUhU7qsi30uP9aqovL615FnnU9+fuPugy5XQqmXdGFpxdupkvrUGIiQ/lkQzK2qlrG9at7+eHJTzYRHujDPWN6AzBlcBfumbuKj75P4vKurVi9J43kTCt/vG4AACaTiamDu/D+2ngiwyy0DPHj7ZX7CAvwZUhMJABtmgcysHNL5izeyuzJ/amx23l56XZG9owmPND3/OyI82jHB98z5slp5CRlcDw+nb7Th+Lh40n8F9sAGPPkNEpzi1n/969c5us+cQCHvos/LTjczG5MeGEmzbpE8NlD72Jyd8P35FWwragce00tAL2nDCZr31Gqy6uIHtCJoQ+NZ92bX1FZ2vBzhovZ1LgOPPvVLrq0CKJby2AW7jiMrbqWa7pHAfD0sp2EWXy4e2g3AKb0bc+9H2/g422HuLx9c1YnZZGcXcijo3u5LLessppvU45x3/CY09YZn2Ul4VgBfaLD8PU0E59l5Y218VwdE0mAt+dZ7/OvcUGES+aa3XgF+dPttrF1H1EezGTD7//pfL3Yt3kwP/5cx9PiQ59Hb8Q7JIDqknIKUjL49s7XKEmtv5r2Dg2gx/3X4h1ioSK/mPTl20j61wqX9e59Ywk97A4GPvs73DzM5GxLZvdLC89Np5uQkb3aUFBWyTsr95JfYqNjq2BevXW4c3gqp7AMN1P9s6oebcJ5cvog5i/fy9zle4gMs/DCLUOc37gAzBjWjYqqGp7/dCultip6tGnGq7cOd37jAvDEtEG8vHQ7D8xfg8nNxLDYSB4+g2GDi1HKqj34Bvsz6K7R+IYGkHsgi8X3z6fcWveNS0AL13MAIDg6nNa927HonrmnLc8/PJAOw2IB+M0nf3CZtuCOf5Cxs+418xYxkVx+5yg8fL2wpp5g1bOLGrwLuhSM7BpBYXkl72xIxlpWScdmAbw8ZaBzWCynuALTj86D7q1DeGJ8X+avT2LeuiRaB/sxZ/IA5zcuP1idlIXDAVd1a33aOj3c3VidlMV7G5OpqrXTKtCXqf3ac2Nc+7PbWQNcEB9RSp3z9RGl1DtfH1FKvfP1EaXUu2g+ohQRkQuLwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcOYzLSyqcT+b2yFn4L2nCs/3Jlzy8qrt53sTLnluof7nexPkDOjORUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMZz7fG3Cmul4/iNibRuATaqHg4DE2v7yEvMT0Rus9/b3pe/c1RA/rgVeAL6XZVra+upTMTUkA9L5tFL1vH+0yT2FqDkumPu/SFh4bTd+7ryE8JgqH3YH1QBYrHpxHbWW18Z1swnrdMIi4W4bjF2oh9+Ax1rz4GdkJDe//qfPuIbJfh9Paj2xIZMmD7+BmdmPw3WNpO7grQREhVJbaSNt6gHVvfkVZXrGzvlmXCIbcP44WMVE4au0cWLuP7175nOqKqrPWz6Zs4NTBDJ05Av8wC8cPHOOLOZ+SGd/4OTBoxlAGThlEUIsgygrLiF+1l+WvL6OmqgaAtn3bMWTmCCK6RhLQLJD/PPguid/uP205V90zhrjrBuJj8SF1z1GWPrOI/PS8s9bPpmzxphQ+/D4Ja0kFHVoG8/DEfsREhTVav2ZfGvNX7CO7oJTWYRbuHdOby7tGOKc7HA7eXrmPL7YdoqSimh5twpl9bRyR4QHOmqLySl5ZuoMNSZm4mUwM6x7FrAl98fXyOKt9/bUuiHBpO7IX/R+cxKYXFpGbkEbMjUMZ9fqdfDplDraC0tPq3czujHrzbmwFpaz90/uU5xbi3yKEqtIKl7qCw8dZft8/nb/ttXaX6eGx0Yx6/U72/XsNW15agr22ltCOETjsrnUXu85X9WLYwxNZ/dwijsen02f6EK7/+x28N/l5yhvY/58/8j5uHu7O3z6Bvvzm4z+QsnovAGZvT5p1iWDLOys5ceAY3hZfRjwyiWtfvZUPbn4VAL+wAG54625SVu1mzYtL8PLzZvjvJzHmiWl88ei/z03Hm5Aeo3oz7pFJfPb0QjL2pzFoxlBunXsXL014jjLr6ceg59g+jH5wHIsf/5j0PamERYdzw9PTcTjgq5eWAuDh48XxlGPs+GwrN792a4PrHfrbK7l8+hAW/eVDrFn5XH3fWH439y5enfS8M6QuFav3pPLGl7uYPbk/MVFhLFifzKx3v+WTR8YT4u99Wv2+1Fwe/2gjd43uxeCuEazYk8qj/1nH+w+OoX2LIAA++C6RRRtT+OvUy2gV4s/8Fft46N1v+ej34/A6eQ498fFG8ottvHH7ldTU2nlm4Rae/3QrT00ffC67/4tdEMNisdOGkfL5Zg4u20bh0Rw2Pr+IGlsVncYPaLC+4/gBeAX4svqRdzmx7yilxwvI3n0Y68FjLnX2WjsV1hLnX2VRmcv0AbMmkbhwPfv+s4bCo9kUp+dydM0e7NW1Z62vTVG/GUPZ/9kW4r/cTv7RHFY9t5hqWzWxE/s3WG8rLqc8v8T5Fz2gM9W2ag6sqguXqlIbi++dR8qqvRSk5XI8Po01LyyhRbdILCdPuvZXdMNeU8vq55dQkJZLdmIGq+YsptPIngS1bvxK8WI1+JZhbPt0Mzs/38aJIzksfXoRVRVV9JvU8DkQ3bMtaXuOsvfrXRQcs3Jwcwp7v9lFZGyUs+bAhiRW/v1rEtaefrfyg0EzhrD27ZUkfhdP9sHjLHjsQwLCA+k2orvhfWzqPl6fzIQBHRgX1562zQOZPbk/Xh7uLNt+uMH6hRuSGdCpJTOGdaNN80DuHNWTzhHBLN6YAtTdtSzYkMzMK2MZEhNJh5bB/G3qZeQVl7MuIQOA1JwitqQc50/XDyAmKoyebZvx8KR+rN6bRm5R+Tnr+/+iyYeLm9md0C6tObbtQH2jw8Gx7QcJ7x7d4DxRQ2I4sT+Vy2dfz7RvnuLaj2bT4zcjMbmZXOoCIsO4cdkT3LDkLwx9cgZ+zYOc07yD/WkW24YKaynXvP0A0755ijH/vJfmPduejW42WW5md5p3aU3aKfs/fdsBWnVvc0bL6D5pAMkrd1Nta3w4y9PfG4fdTmVJ3d2lu6eZ2uoacDicNTW2uqHIiN6X1jFwN7sT0bU1h7bUHwOHw8GhrQeI7tmmwXnS9h4lomskrU+GSUhEKJ2v6EbyhsQzXm9IRCgB4YEu660stZGxP63R9V6sqmtqScmyEtehhbPNzc1EXMcWxKc1PEQYn55HXMeWLm0DOrUi/uSQ4jFrKfklNuI61i/T38eTbpFhzmXuT8/D4uNJ18hQZ01chxa4mUwkZOQb1r+zocmHi1eQH25mdyqsJS7tFdYSfEMCGpzH0iqUNiN6YnIzsXLWfPa8t5LYm4bR87dXO2tyE9JY/9THrHhoHpteWIR/qxCumXc/Zl+vumVE1B3M3reP4sDnW1jx4DzyU7IY/fd7CIi8dK6cfU7u/7J81/1fll+CX5jlZ+dvERNFeIeW7F+6tdEad08zQx4YR9KK3VSVVQKQvv0gfmEBxN08HDezO14WH4bcfw1QN2R2KfEN9sPd7E7pKcegNL8E/0b2xd6vd7HqrW+4698P8OzOl5n9zV85suMQ372z+ozX63/y+Da43tBL6xgUllVSa3cQYnEd/grx9ya/pKLBefJLbKcNl9XV25zT69p8XGss9cvML6kg2M/LZbrZ3Y0AH0+sjay3qbggnrn8UiY3E7aCUjbOWYjD7iA/ORPf8EC6zxjBnndXAJC5OdlZX3DoOLkJaUz5/G+0vbIXB7/cislUd5eT8tkmDi7bBsC2A1m06teRjuMHsPOtr859xy5A3ScOIPfgsUYf/ruZ3Rj//C2YTCZWz1nsbM8/ksM3j3/M8FkTuOK+sdjtDnZ/sr7ugb/d0eCypF67fh0YfttIPn92Men70wiLDGP8o5MZccfVrJ2/8nxvnlwCmny4VBaWYa+pxSfE9SrZJ8RCubW4wXnK84px1Nhx/Og/oaLUHHzDAnAzu2OvOf2ZSVWpjaL0XOddSfnJt5YKj+a41BWm5uDfPPhX9elCUnFy//uFuu5/v1ALZXkljcxVx8Pbky6jerFx7vIGp9cFy28IaBnCwrvect61/CB5+S6Sl+/CN8S/7g0xB/S9aSiFWU17OMBo5QVl1NbU4n/KMfAPtVCa1/A5cNV9Y9i1bAfbl2wBIOfgcTx8PJn8t6l8+/YqHI6fD+jSk8fXP9RCyY/W4x9q4XhK1v/anQtSkJ8X7m4mrCfvNn5gLbURavFpcJ5QizfW0obqvZ3T69oqCAuoX4a1xEanVsEna3woOOW8qKm1U1xRRUgj620qmvywmL2mlvzkTFrFdapvNJloFdeR3P1pDc5zYt9RLK3DwFT/jCUgqhnluUUNBguA2ceTgIhQKk6eRKXHrZSdKCQwuplLXWBUOKXZ1l/ZqwuHvaaWnORMouI61jeaTETFdeTY/tSfnLfTVT1x9zCT+PXO06b9ECzBkWEsuvuf2H7i4WS5tZTqiio6X92L2qpq0rak/K/duSDV1tSSlZRJhwH1x8BkMtFhQCfS9qY2OI+Ht6fLxRVQ/9vUwAwNsGblU5xb5LJeLz8vIrtHN7rei5WH2Z3OESHsOJTtbLPbHew4lE1sdMPD5LFRYS71ANsOHif25KvLrUL8CbV4s+Ng/QVsma2axIw85zK7R4VRUlFFcmb9BdXOwznYHQ5ifvQcpilq8uECEP/xd3SaOJAOY+MIbNOMyx+9HrO3JweW1Y3jD3l8On3vucZZn/zpJrwCfRn48LUERIbTelA3es4cSdLiDc6auAcm0KJ3e/xbBtOsexuufOF32O0Ojqzc5azZ/+G3dJtyBW1G9MTSOow+d44hMLoZB75o/PnBxWjHB9/T49qBxIzrR0ibZlz1p+vx8PEk/ou64cIxT07jivuuOW2+7hMHcOi7+NOCw83sxoQXZtK8a2u++suHmNzd8A214Btqwc1c/wpz7ymDadYlguCocHrdMIgrH53Mur9/TeUpV4OXgg3/+Y646y6jz4Q4wts2Z9JfbsDTx5OdJ59lTXn2JkY9MM5Zn/x9AgOnDKLH6N4ER4TQYWAnrrp3DEnfJzhDxtPHk5adI2jZue67i5CIEFp2jiDw5Bt7ABs/WMeIO66m67AYmndsyZRnZ1CcW0TiT7xhdrGadkUXvth2iK92HCE1p4gXP9uGraqWcf3aAfDkJ5t465vdzvopg7uwJeUYH32fROqJIt5ZuY/kTCvXD+oM1F0gTB3chffXxrM+IZNDxwt4asEmwgJ8GRITCUCb5oEM7NySOYu3kpCex97UE7y8dDsje0YTHuh77nfCL9Dkh8UAjq7eg3eQP33uGI1PaADWA1msfGgetpPv9/s1D3a5Sis7UciKB+YyYNYkJn34COW5RSR8so79/13jrPFrFsiwp2/GK9APW2EpOXuPsOzW17AV1r+OnPjJOsyeHvR/aCJeAb5YDx5jxQNzKbnEhmVSVu3BN9ifQXeNxjc0gNwDWSy+fz7lJ/d/QIvg04ZZgqPDad27HYvumXva8vzDA+kwLBaA33zyB5dpC+74Bxk7617tbBETyeV3jsLD1wtr6glWPbuowbugS8G+FbvxC/bjqnvGYAkL4FhKFu/dPY/Sk8cgqIXrObB2/kocDgdX3zeWwGaBlBWUkfR9PCve/NpZ0zomijveu8/5e9zsawHY+fk2Fv31IwC+/9caPE8Op3lbfEjdfYR/3T3vkvvGBWBkrzYUlFXyzsq95JfY6NgqmFdvHe4cnsopLMPtR6MlPdqE8+T0Qcxfvpe5y/cQGWbhhVuGOL9xAZgxrBsVVTU8/+lWSm1V9GjTjFdvHe78xgXgiWmDeHnpdh6YvwaTm4lhsZE8PLHfOev3/8rkOJPBV+C9AbPO9rbIz7DWnOF4hpw1edWX1ge0TdHsp0PO9yZc8kIm/u1nay6IYTEREbmwKFxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzJ4XA4zqTQ+vlTZ3tb5Ge8+Ffr+d6ES15uTfX53oRLXk6V7XxvwiVv2aF3f7ZGdy4iImI4hYuIiBhO4SIiIoZTuIiIiOEULiIiYjiFi4iIGE7hIiIihlO4iIiI4RQuIiJiOIWLiIgYTuEiIiKGU7iIiIjhFC4iImI4hYuIiBhO4SIiIoZTuIiIiOEULiIiYjiFi4iIGE7hIiIihlO4iIiI4RQuIiJiOIWLiIgYTuEiIiKGU7iIiIjhFC4iImI4hYuIiBhO4SIiIoZTuIiIiOEULiIiYjiFi4iIGE7hIiIihlO4iIiI4RQuIiJiOIWLiIgYTuEiIiKGU7iIiIjhFC4iImI4hYuIiBhO4SIiIoZTuIiIiOEULiIiYjjz+d6AM7V4Uwoffp+EtaSCDi2DeXhiP2KiwhqtX7Mvjfkr9pFdUErrMAv3junN5V0jnNMdDgdvr9zHF9sOUVJRTY824cy+No7I8ABnTVF5Ja8s3cGGpEzcTCaGdY9i1oS++Hp5nNW+NkUDpw5m6MwR+IdZOH7gGF/M+ZTM+PRG6wfNGMrAKYMIahFEWWEZ8av2svz1ZdRU1QDQtm87hswcQUTXSAKaBfKfB98l8dv9py3nqnvGEHfdQHwsPqTuOcrSZxaRn5531vrZlA2fNoTRvxtJYFgAGSlZfPTsQo7uT2uw1t3sxtjbR3H5xAEENw8i+2gOi1/5nPgNib9omWZPM1NnT6b/2L6YPT1I2JDIB08voDi/5Kz2tSmKievEdbePon1MG0KbB/HMXX9ny+rdPzlP9wGdufXPU4nu2Irc41YW/OMr1izZ6FJzzYzhTL5tNMHhgRxNymDeUx9xYN9R53QPTzO3/nkqQ67pj4enmV3rE/jn4x9QmF98VvpplAvizmX1nlTe+HIXt47szvsPjqVjy2Bmvfst1lJbg/X7UnN5/KONjI9rz78fHMuQmEge/c86DmcXOms++C6RRRtTmD25P+/ePwofTzMPvfstldW1zponPt7I0Zwi3rj9Sl767TD2HDnB859uPcu9bXp6jOrNuEcmsXruct6c+hLHU7K4de5d+IX4N1jfc2wfRj84jtVzl/PKpOf59PFP6DGqN6MeGOes8fDx4njKMT5/bnGj6x362yu5fPoQlj69iH/c9CrVFVX8bu5dmD0vmGsiw8SN7sPURyfzxVtf8+T1z5ORnMms+fdhaeQYXPvAeIZOGcxHzy3iL+Of5rsFG7j3jduJ6tr6Fy3zxj9eT8/h3fnnrHd58ZZXCWoWyD2v337W+9sUeft4ciQpk7lPfHBG9c1bh/H42w+yf0sy949/ki/eX80Dz/2GPlfEOGuuGBvHbX+eysdvfsGDE5/kaHIGT/1rFoEhFmfN7Y/dSP8RPXn+/n/yx+kvEto8iD+/dY/h/TPaBREuH69PZsKADoyLa0/b5oHMntwfLw93lm0/3GD9wg3JDOjUkhnDutGmeSB3jupJ54hgFm9MAeruWhZsSGbmlbEMiYmkQ8tg/jb1MvKKy1mXkAFAak4RW1KO86frBxATFUbPts14eFI/Vu9NI7eo/Jz1vSkYfMswtn26mZ2fb+PEkRyWPr2Iqooq+k0a0GB9dM+2pO05yt6vd1FwzMrBzSns/WYXkbFRzpoDG5JY+fevSVh7+t3KDwbNGMLat1eS+F082QePs+CxDwkID6TbiO6G97Gpu3rmlaxbtImNn23h+OFs/vvkJ1TZqhg8+bIG6y+b0J+v5q9g/7oE8jLz+W7BevavS+DqmVee8TJ9/L254rrLWPDCEpK3HiAtMYP3HvuAjn3a065Hm3PR7SZl57p4Pnj1Mzav+um7lR+MmTaMnMw83p2zkMzDx1n237VsXL6Tib+9ylkz6XdXs2LBOlZ/upGMQ8f5x1//S2VFFVfdMBgAX38frrrhCt59bgH7tiRzOCGN1x59j259O9K5V7uz0k+jNPlwqa6pJSXLSlyHFs42NzcTcR1bEJ/W8PBIfHoecR1burQN6NSK+JPDKcespeSX2IjrWL9Mfx9PukWGOZe5Pz0Pi48nXSNDnTVxHVrgZjKRkJFvWP+aOnezOxFdW3NoywFnm8Ph4NDWA0T3bNPgPGl7jxLRNZLWJ8MkJCKUzld0I/mUIZmfEhIRSkB4oMt6K0ttZOxPa3S9Fyt3D3eiu0WStCXZ2eZwOEjcnEz7Rv6DMXuaqa6sdmmrqqymY5/2Z7zM6JgozB5mEjfX12QfzSH/mJX2vdoa1r+LVZfe7dmz0fXf/K718XTpXXcMzB7udIiNZs/GJOd0h8PBnk2JzpoOsdF4eJpdlpN5JJsTWfnOmqaqyY8vFJZVUmt3EGLxdmkP8fcm7UTDY475JTZC/E+vzy+xOafXtfm41li8yS+pOFlTQbCfl8t0s7sbAT6eWE/WXAp8g/1wN7tTesoYe2l+CeFtmzc4z96vd+EX5M9d/34AEybcPdzZsnAj372z+ozX6x9mca7n1PX6hwY0NMtFyxLkj7vZneI8131RnF9Cy3YtGpwnfkMSV8+8kgM7D5GbnkfXgZ3pM7IXbu6mM15mYFgA1VXVVJzy770or5jAsEvrGPwvgsMDTnsuUphXjJ/FF08vD/wD686thmpat2t5chmBVFdVU3bKMSjMKyI4LPDsduBXavLhIheedv06MPy2kXz+7GLS96cRFhnG+EcnM+KOq1k7f+X53rxLwsdzFjPzqek8u+xvOBwOcjPy2PjZ5kaH0USM1uTDJcjPC3c3E9YS14f31lIboRafBucJtXif9rC/rt7bOb2urYKwgPplWEtsdGoVfLLGh4KySpdl1NTaKa6oIqSR9V6MygvKqK2pxT/U4tLuH2qhNK/hO8er7hvDrmU72L5kCwA5B4/j4ePJ5L9N5du3V+FwOH52vaUnr6j9Qy2U/Gg9/qEWjqdk/a/duSCVFJZSW1NLQJjrMQgItVDUyDEoLSjl7/fPx+xpxj/Ij8ITRVz/8ERyM/PPeJlFecV4eHrgY/FxuXsJDAtodL1SryC3mKBT7rKDwgIoKymnqrKa4oISamtqG6wpyCs6uYwiPDw98LP4uNy9BIUFOmuaqib/zMXD7E7niBB2HMp2ttntDnYcyiY2uuFXkWOjwlzqAbYdPE7syVeXW4X4E2rxZsfBHOf0Mls1iRl5zmV2jwqjpKKK5Mz65ys7D+dgdziI+dFzmItdbU0tWUmZdBjQ0dlmMpnoMKATaXtTG5zHw9sTh901QJy/TWe2XmtWPsW5RS7r9fLzIrJ7dKPrvVjVVteSlphB14GdnW0mk4muAztzeM+Rn5y3pqqGwhNFuJvd6HN1b/as3XfGy0xLSKemuoZuP6pp3qYZoa1COLznKPLTkncfpuflXV3aeg3qRvLuuheRaqprORSf5lJjMpnoeXlXZ82h+DSqq2roeXk3Z01E2+Y0iwh11jRVTf7OBWDaFV14euFmurQOJSYylE82JGOrqmVcv7oHj09+sonwQB/uGdMbgCmDu3DP3FV89H0Sl3dtxeo9aSRnWvnjdXVvN5lMJqYO7sL7a+OJDLPQMsSPt1fuIyzAlyExkQC0aR7IwM4tmbN4K7Mn96fGbuflpdsZ2TOa8EDf87MjzpMN//mOG56ZTmZiBhn70xk8YyiePp7sXFr3WvaUZ2+iKKeIFW8sAyD5+wQG3zyMY8mZZOxPIzQyjKvuHUPS9wnOkPH08SQ0Kty5jpCIEFp2jqC8qIyik6+Mb/xgHSPuuJq89FysWVauvncsxblFJP7EG2YXq5Xvr+HWObeQGp/O0f2pjLxlBF4+Xmz8rO7u8NY5t1BwopAlr34BQNsebQhuFkh6cibBzYOYeO81uJlMfPPuqjNeZkWpjfWfbmbqo9dRWlSGrdTG9MemcGj3EY7sSz3n++B88/b1omV0M+fv5pFhtO0aSWlhGbnHrfzmD5MJbR7MK4+8C8A3H3/HuJtH8NvZ17Nq8QZ6XNaVK8bG8eTtrzuXsfS9lcz6v1s5uD+VA/uOMnHmSLx9vFi9uO5bmPLSClYtWs9tf55KSVEp5SU27np8Okm7DpHyMxcW59sFES4je7WhoKySd1buJb/ERsdWwbx663Dn8FROYRlupvpL4h5twnly+iDmL9/L3OV7iAyz8MItQ2jfIshZM2NYNyqqanj+062U2qro0aYZr946HC8Pd2fNE9MG8fLS7Twwfw0mNxPDYiN5eGK/c9bvpmLfit34Bftx1T1jsIQFcCwli/funkeptRSAoBbBLncqa+evxOFwcPV9YwlsFkhZQRlJ38ez4s2vnTWtY6K44737nL/Hzb4WgJ2fb2PRXz8C4Pt/rcHz5HCat8WH1N1H+Nfd85wfYl5Kti/fhSXEwqT7xxEQZiEjOYtX7/yH82PGkJaux8DD08y1D44nvHUYtvJK9q9L4J1H/+0yvPVzywT45PnFOOx27n39dsweZuI3JvHB0wvOXcebkI7d2zDnw9nO37c/diMAqz/dyGuPvkdweBDhrUKc03My83jy9te57bEbmTBzJHnZBbzx53+za32Cs2b919sJDLUw46FJBIcHcCQxg7/97lWXh/xvP/sJdoeDP//93pMfUcbz1uNn9q3N+WRynMkAOGD9/KmzvS3yM178q/V8b8IlL7em+ueL5KzKqWr442k5d5Ydevdna5r8MxcREbnwKFxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzCRUREDKdwERERwylcRETEcAoXERExnMJFREQMp3ARERHDKVxERMRwChcRETGcwkVERAyncBEREcMpXERExHAKFxERMZzJ4XA4zvdGiIjIxUV3LiIiYjiFi4iIGE7hIiIihlO4iIiI4RQuIiJiOIWLiIgYTuEiIiKGU7iIiIjhFC4iImK4/wfmKxeCMItoHAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_function = np.max(Q_values, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "plot_values(ax, value_function)\n",
    "format_plot(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "\n",
    "- What happens when the epsilon is set to 1? Why?\n",
    "\n",
    "- What happens when the epsilon is set to 0? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reinforcement learning is an approach to solving MDPs when the underlying transition and reward models are not known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order to learn the optimal action, the RL agent interacts with the environment. These interactions provide feedback based on which the agent takes future actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Q Learning is value-based algorithm that updates the Q values of state-action values based on interaction with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q_{k+1}(s, a) = Q_k(s, a) + \\alpha \\cdot \\big [  r + \\gamma \\cdot \\max_{a'} Q_k(s, a') -  Q_k(s', a) \\big ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The exploration/exploitation trade-off is important aspect in RL to ensure you sufficiently explore the policy space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Brockman, G.. OpenAI Gym. arXiv (2016). https://arxiv.org/pdf/1606.01540.pdf\n",
    "\n",
    "Richard S. Sutton & Andrew G. Barto. Reinforcement learning : an introduction. (2020)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "df9c06b5e8521cf4ac10b32b5194d6e2193b759c220900fa5a35003364e8f855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
